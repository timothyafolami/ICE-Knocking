{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "from loguru import logger\n",
    "import sys\n",
    "import joblib\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from typing import Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>\")\n",
    "logger.add(\"ml_engine_knock_analysis.log\", rotation=\"500 MB\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "def create_output_dirs():\n",
    "    \"\"\"Create directory structure for outputs\"\"\"\n",
    "    dirs = ['outputs', 'outputs/predictions', 'outputs/forecasts', 'outputs/feature_importance', 'outputs/models']\n",
    "    for dir_path in dirs:\n",
    "        os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering:\n",
    "    def __init__(self):\n",
    "        self.scalers = {}  # Store scalers for each feature\n",
    "        self.feature_scaler = MinMaxScaler()  # For scaling all features\n",
    "        \n",
    "    def prepare_features(self, df: pd.DataFrame, target_feature: str) -> tuple:\n",
    "        \"\"\"Prepare features for modeling using all features except the target\"\"\"\n",
    "        # Create a copy of the dataframe\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Create time-based features\n",
    "        df_processed['hour'] = df_processed['Timestamp'].dt.hour\n",
    "        df_processed['day_of_week'] = df_processed['Timestamp'].dt.dayofweek\n",
    "        df_processed['month'] = df_processed['Timestamp'].dt.month\n",
    "        df_processed['is_weekend'] = df_processed['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Get numeric columns (excluding Timestamp)\n",
    "        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col != 'Timestamp']\n",
    "        \n",
    "        # Scale numeric features first\n",
    "        numeric_data = df_processed[numeric_cols].values\n",
    "        scaled_numeric = self.feature_scaler.fit_transform(numeric_data)\n",
    "        df_processed[numeric_cols] = scaled_numeric\n",
    "        \n",
    "        # Create lag features for all numeric columns\n",
    "        for col in numeric_cols:\n",
    "            # Add lag features (1-3 lags)\n",
    "            for lag in range(1, 4):\n",
    "                df_processed[f'{col}_lag_{lag}'] = df_processed[col].shift(lag)\n",
    "            \n",
    "            # Add rolling features\n",
    "            for window in [10, 30, 60]:\n",
    "                df_processed[f'{col}_rolling_mean_{window}'] = df_processed[col].rolling(window=window).mean()\n",
    "                df_processed[f'{col}_rolling_std_{window}'] = df_processed[col].rolling(window=window).std()\n",
    "        \n",
    "        # Drop NaN values\n",
    "        df_processed = df_processed.dropna()\n",
    "        \n",
    "        # Store scaler for the target feature\n",
    "        self.scalers[target_feature] = MinMaxScaler()\n",
    "        target_values = df_processed[target_feature].values.reshape(-1, 1)\n",
    "        df_processed[target_feature] = self.scalers[target_feature].fit_transform(target_values)\n",
    "        \n",
    "        # Prepare X (features) and y (target)\n",
    "        X = df_processed.drop(columns=['Timestamp', target_feature])\n",
    "        y = df_processed[target_feature]\n",
    "        \n",
    "        return X, y, df_processed['Timestamp']\n",
    "    \n",
    "    def inverse_transform_target(self, target_feature: str, values: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Inverse transform the target feature values\"\"\"\n",
    "        if target_feature in self.scalers:\n",
    "            return self.scalers[target_feature].inverse_transform(values.reshape(-1, 1)).ravel()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, top_n_models: int = 2):\n",
    "        self.models = {}  # Dictionary to store models for each feature\n",
    "        self.top_n_models = top_n_models\n",
    "        self.model_metrics = {}\n",
    "        self.feature_names = {}  # Store feature names for each model\n",
    "        \n",
    "    def get_models(self) -> Dict:\n",
    "        \"\"\"Get dictionary of models to train\"\"\"\n",
    "        return {\n",
    "            'xgb': XGBRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'lr': LinearRegression(\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def train_models(self, X_train: pd.DataFrame, y_train: pd.Series, feature_name: str):\n",
    "        \"\"\"Train multiple models for a feature and select the best ones\"\"\"\n",
    "        logger.info(f\"Training models for {feature_name}\")\n",
    "        \n",
    "        # Store feature names and convert to numpy arrays\n",
    "        self.feature_names[feature_name] = X_train.columns.tolist()\n",
    "        X_train_array = X_train.to_numpy()\n",
    "        y_train_array = y_train.to_numpy()\n",
    "        \n",
    "        # Initialize models dictionary for this feature\n",
    "        self.models[feature_name] = {}\n",
    "        self.model_metrics[feature_name] = {}\n",
    "        \n",
    "        # Train all models\n",
    "        for model_name, model in self.get_models().items():\n",
    "            logger.info(f\"Training {model_name} for {feature_name}\")\n",
    "            model.fit(X_train_array, y_train_array)\n",
    "            self.models[feature_name][model_name] = model\n",
    "            \n",
    "            # Get feature importance for tree-based models\n",
    "            if model_name in ['xgb', 'rf']:\n",
    "                importance = dict(zip(self.feature_names[feature_name], model.feature_importances_))\n",
    "                importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "                \n",
    "                # Plot feature importance\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.bar(importance.keys(), importance.values())\n",
    "                plt.title(f'{feature_name} - Top 10 Feature Importance ({model_name.upper()})')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'outputs/feature_importance/{feature_name}_{model_name}_importance.png')\n",
    "                plt.close()\n",
    "        \n",
    "        logger.info(f\"Completed training all models for {feature_name}\")\n",
    "    \n",
    "    def evaluate_models(self, X_test: pd.DataFrame, y_test: pd.Series, feature_name: str) -> Dict:\n",
    "        \"\"\"Evaluate all models and select the best ones\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if feature_name in self.models:\n",
    "            X_test_array = X_test.to_numpy()\n",
    "            y_test_array = y_test.to_numpy()\n",
    "            \n",
    "            for model_name, model in self.models[feature_name].items():\n",
    "                pred = model.predict(X_test_array)\n",
    "                metrics = {\n",
    "                    'r2_score': r2_score(y_test_array, pred),\n",
    "                    'mse': mean_squared_error(y_test_array, pred),\n",
    "                    'mae': mean_absolute_error(y_test_array, pred)\n",
    "                }\n",
    "                results[model_name] = metrics\n",
    "                self.model_metrics[feature_name][model_name] = metrics\n",
    "        \n",
    "        # Select top N models based on MSE\n",
    "        if feature_name in self.model_metrics:\n",
    "            sorted_models = sorted(\n",
    "                self.model_metrics[feature_name].items(),\n",
    "                key=lambda x: x[1]['mse']\n",
    "            )[:self.top_n_models]\n",
    "            \n",
    "            # Keep only the best models\n",
    "            self.models[feature_name] = {\n",
    "                model_name: self.models[feature_name][model_name]\n",
    "                for model_name, _ in sorted_models\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Selected top {self.top_n_models} models for {feature_name}:\")\n",
    "            for model_name, metrics in sorted_models:\n",
    "                logger.info(f\"  {model_name}: MSE = {metrics['mse']:.4f}, R² = {metrics['r2_score']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def generate_forecast(self, X: pd.DataFrame, feature_name: str, \n",
    "                         last_timestamp: pd.Timestamp, feature_eng: FeatureEngineering,\n",
    "                         n_steps: int = 20160) -> pd.DataFrame:\n",
    "        \"\"\"Generate forecast for the next n_steps (2 weeks for minute data)\"\"\"\n",
    "        logger.info(f\"Generating {n_steps}-step forecast for {feature_name}\")\n",
    "        \n",
    "        # Create future timestamps\n",
    "        future_timestamps = pd.date_range(\n",
    "            start=last_timestamp + timedelta(minutes=1),\n",
    "            periods=n_steps,\n",
    "            freq='min'\n",
    "        )\n",
    "        \n",
    "        forecast_df = pd.DataFrame({'Timestamp': future_timestamps})\n",
    "        \n",
    "        # Get buffer size and initialize buffer\n",
    "        buffer_size = max([int(col.split('_')[-1]) for col in X.columns if 'rolling_' in col] + [3])\n",
    "        buffer_df = X.iloc[-buffer_size:].copy()\n",
    "        \n",
    "        # Ensure target feature is in buffer\n",
    "        if feature_name not in buffer_df.columns:\n",
    "            original_df = pd.read_csv(\"../data/engine_knock_data_minute.csv\")\n",
    "            original_df['Timestamp'] = pd.to_datetime(original_df['Timestamp'])\n",
    "            buffer_df[feature_name] = original_df[feature_name].iloc[-buffer_size:].values\n",
    "        \n",
    "        # Get feature order used during training\n",
    "        feature_cols = self.feature_names[feature_name]\n",
    "        \n",
    "        for model_name, model in self.models[feature_name].items():\n",
    "            logger.info(f\"Generating forecast using {model_name} model\")\n",
    "            scaled_predictions = []\n",
    "            \n",
    "            for step in range(n_steps):\n",
    "                try:\n",
    "                    # Get current features in correct order\n",
    "                    current_features = buffer_df[feature_cols].iloc[-1:].to_numpy()\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    pred = float(model.predict(current_features)[0])\n",
    "                    \n",
    "                    # Clip predictions to valid range (0 to 1 for scaled data)\n",
    "                    pred = np.clip(pred, 0, 1)\n",
    "                    scaled_predictions.append(pred)\n",
    "                    \n",
    "                    # Update buffer for next step\n",
    "                    new_row = buffer_df.iloc[-1].copy()\n",
    "                    new_row[feature_name] = pred\n",
    "                    \n",
    "                    # Update lag features\n",
    "                    for lag in range(1, 4):\n",
    "                        lag_col = f'{feature_name}_lag_{lag}'\n",
    "                        if lag == 1:\n",
    "                            new_row[lag_col] = pred\n",
    "                        else:\n",
    "                            new_row[lag_col] = buffer_df[feature_name].iloc[-(lag-1)]\n",
    "                    \n",
    "                    # Update rolling features\n",
    "                    for col in feature_cols:\n",
    "                        if 'rolling_mean' in col:\n",
    "                            window = int(col.split('_')[-1])\n",
    "                            new_row[col] = buffer_df[feature_name].rolling(window).mean().iloc[-1]\n",
    "                        elif 'rolling_std' in col:\n",
    "                            window = int(col.split('_')[-1])\n",
    "                            new_row[col] = buffer_df[feature_name].rolling(window).std().iloc[-1]\n",
    "                    \n",
    "                    # Append new row and maintain buffer size\n",
    "                    buffer_df = pd.concat([buffer_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                    buffer_df = buffer_df.iloc[-buffer_size:]\n",
    "                    \n",
    "                    # Handle numerical issues\n",
    "                    buffer_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                    buffer_df.fillna(method='ffill', inplace=True)\n",
    "                    buffer_df.fillna(method='bfill', inplace=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Forecast error at step {step} for {model_name}: {e}\")\n",
    "                    fallback = scaled_predictions[-1] if scaled_predictions else buffer_df[feature_name].iloc[-1]\n",
    "                    scaled_predictions.append(fallback)\n",
    "            \n",
    "            # Convert scaled predictions to original scale\n",
    "            original_predictions = feature_eng.inverse_transform_target(\n",
    "                feature_name,\n",
    "                np.array(scaled_predictions)\n",
    "            )\n",
    "            \n",
    "            # Store predictions in original scale\n",
    "            forecast_df[f'{model_name}_forecast'] = original_predictions\n",
    "            \n",
    "            # Log forecast statistics (only if predictions are valid)\n",
    "            if not np.any(np.isnan(original_predictions)) and not np.any(np.isinf(original_predictions)):\n",
    "                logger.info(f\"Forecast statistics for {model_name} (original scale):\")\n",
    "                logger.info(f\"  Mean: {np.mean(original_predictions):.2f}\")\n",
    "                logger.info(f\"  Std: {np.std(original_predictions):.2f}\")\n",
    "                logger.info(f\"  Min: {np.min(original_predictions):.2f}\")\n",
    "                logger.info(f\"  Max: {np.max(original_predictions):.2f}\")\n",
    "            else:\n",
    "                logger.warning(f\"Invalid predictions detected for {model_name}, skipping statistics\")\n",
    "        \n",
    "        # Plot forecasts\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        for model_name in self.models[feature_name]:\n",
    "            if not np.any(np.isnan(forecast_df[f'{model_name}_forecast'])) and \\\n",
    "               not np.any(np.isinf(forecast_df[f'{model_name}_forecast'])):\n",
    "                plt.plot(forecast_df['Timestamp'], \n",
    "                        forecast_df[f'{model_name}_forecast'],\n",
    "                        label=f'{model_name.upper()} Forecast',\n",
    "                        linestyle='--',\n",
    "                        alpha=0.7)\n",
    "        \n",
    "        plt.title(f'{feature_name} - 2-Week Forecast (Original Scale)')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel(feature_name)\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'outputs/forecasts/{feature_name}_forecast.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return forecast_df\n",
    "\n",
    "    def plot_predictions(self, y_true: pd.Series, predictions: Dict[str, np.ndarray], \n",
    "                        feature_name: str, timestamps: pd.Series, feature_eng: FeatureEngineering):\n",
    "        \"\"\"Plot predictions from all models\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Convert y_true to numpy array if it's a Series\n",
    "        y_true_array = y_true.to_numpy() if isinstance(y_true, pd.Series) else y_true\n",
    "        \n",
    "        # Inverse transform the values for plotting\n",
    "        y_true_original = feature_eng.inverse_transform_target(feature_name, y_true_array)\n",
    "        predictions_original = {\n",
    "            model_name: feature_eng.inverse_transform_target(feature_name, pred)\n",
    "            for model_name, pred in predictions.items()\n",
    "        }\n",
    "        \n",
    "        # Plot actual values\n",
    "        plt.plot(timestamps, y_true_original, label='Actual', alpha=0.7)\n",
    "        \n",
    "        # Plot predictions from each model\n",
    "        for model_name, pred in predictions_original.items():\n",
    "            plt.plot(timestamps, pred, label=f'{model_name.upper()}', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.title(f'{feature_name} - Actual vs Predicted')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel(feature_name)\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'outputs/predictions/{feature_name}_predictions.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Log prediction statistics (using original scale)\n",
    "        logger.info(f\"Prediction statistics for {feature_name}:\")\n",
    "        for model_name, pred in predictions_original.items():\n",
    "            metrics = {\n",
    "                'r2_score': r2_score(y_true_original, pred),\n",
    "                'mse': mean_squared_error(y_true_original, pred),\n",
    "                'mae': mean_absolute_error(y_true_original, pred)\n",
    "            }\n",
    "            logger.info(f\"  {model_name}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                logger.info(f\"    {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create output directories\n",
    "    create_output_dirs()\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    logger.info(\"Loading data from CSV file\")\n",
    "    df = pd.read_csv(\"../data/engine_knock_data_minute.csv\")\n",
    "    logger.info(f\"Loaded data shape: {df.shape}\")\n",
    "    \n",
    "    # Drop unnecessary columns and convert timestamp\n",
    "    df = df.drop(columns=['Knock', 'IgnitionTiming'])\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    # Initialize feature engineering and model trainer\n",
    "    feature_eng = FeatureEngineering()\n",
    "    trainer = ModelTrainer(top_n_models=2)  # Keep top 2 models per feature\n",
    "    \n",
    "    # Process each feature\n",
    "    features = ['RPM', 'CylinderPressure', 'BurnRate', 'Vibration', 'EGOVoltage', 'TempSensor']\n",
    "    results = {}\n",
    "    forecasts = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        logger.info(f\"\\nProcessing feature: {feature}\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y, timestamps = feature_eng.prepare_features(df, feature)\n",
    "        \n",
    "        # Split data maintaining temporal order\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "        y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "        test_timestamps = timestamps.iloc[train_size:]\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        trainer.train_models(X_train, y_train, feature)\n",
    "        feature_results = trainer.evaluate_models(X_test, y_test, feature)\n",
    "        results[feature] = feature_results\n",
    "        \n",
    "        # Generate predictions for plotting\n",
    "        predictions = {}\n",
    "        for model_name, model in trainer.models[feature].items():\n",
    "            predictions[model_name] = model.predict(X_test.to_numpy())\n",
    "        \n",
    "        # Plot predictions\n",
    "        trainer.plot_predictions(y_test, predictions, feature, test_timestamps, feature_eng)\n",
    "        \n",
    "        # Generate and save forecast\n",
    "        last_timestamp = timestamps.iloc[-1]\n",
    "        forecast_df = trainer.generate_forecast(X, feature, last_timestamp, feature_eng)\n",
    "        forecasts[feature] = forecast_df\n",
    "        \n",
    "        # Save forecast to CSV\n",
    "        forecast_df.to_csv(f'outputs/forecasts/{feature}_forecast.csv', index=False)\n",
    "        \n",
    "        # Log results\n",
    "        logger.info(f\"Results for {feature}:\")\n",
    "        for model_name, metrics in feature_results.items():\n",
    "            logger.info(f\"  {model_name}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                logger.info(f\"    {metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Save all models\n",
    "    joblib.dump(trainer.models, 'outputs/models/ml_models.joblib')\n",
    "    logger.info(\"Saved all models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "betzflip-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
